% Sample references (from template)
@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

% Hallucination in Language Models
@article{openai2025gptoss,
  title={gpt-oss-120b \& gpt-oss-20b Model Card},
  author={OpenAI},
  journal={arXiv preprint arXiv:2508.10925},
  year={2025}
}

@article{huang2024survey,
  title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  year={2024},
  publisher={ACM}
}

@article{tonmoy2024comprehensive,
  title={A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models},
  author={Tonmoy, SM and Zaman, SM Mahbub and Joty, Shafiq and Rahman, M Sohel and Hasan, Md Tanvir and others},
  journal={arXiv preprint arXiv:2401.01313},
  year={2024}
}

@article{xu2024inevitable,
  title={Hallucination is Inevitable: An Innate Limitation of Large Language Models},
  author={Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2401.11817},
  year={2024}
}

@inproceedings{lin2021truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20136--20148},
  year={2021}
}

@inproceedings{li2023halueval,
  title={HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
  author={Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={6449--6464},
  year={2023}
}

@inproceedings{niu2024ragtruth,
  title={RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models},
  author={Niu, Yuanhao and Huang, Kaihua and Shi, Bowen and Wang, Shengyu and Xu, Zihao and Yang, Ke and Hong, Guo and Li, Liang and Liu, Zhiyuan and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  pages={10492--10510},
  year={2024}
}

@inproceedings{li2023contrastive,
  title={Contrastive Decoding: Open-ended Text Generation as Optimization},
  author={Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  pages={12286--12312},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

% Ensemble Methods for Hallucination Reduction
@article{zhang2024moellava,
  title={MoE-LLaVA: Mixture of Experts for Large Vision-Language Models},
  author={Zhang, Bin and Yuan, Lin and Wang, Munan and Chen, Zongxin and Yang, Jiaxi and Wu, Daquan and Zhang, Wenwen and Sun, Kuikun and Li, Rongrong and Luo, Songcen and others},
  journal={arXiv preprint arXiv:2401.15947},
  year={2024}
}

% Parallel Architectures and Scaling Laws
@article{chen2025parscale,
  title={ParScale: Parallel Scaling Law for Language Models},
  author={Chen, Yutong and Li, Dawei and Zhang, Yingyu and Ding, Xinyin and Xiao, Chuhan and Zhang, Ruoyu},
  journal={arXiv preprint arXiv:2505.10475},
  year={2025}
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{sardana2024beyond,
  title={Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws},
  author={Sardana, Nikhil and Frankle, Jonathan},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={43414--43454},
  year={2024}
}

@inproceedings{shazeer2017sparsely,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

% Diversity in Neural Networks
@inproceedings{lobacheva2020power,
  title={On Power Laws in Deep Ensembles},
  author={Lobacheva, Ekaterina and Chirkova, Nadezhda and Kodryan, Maxim and Vetrov, Dmitry},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2375--2385},
  year={2020}
}

@inproceedings{ortega2022diversity,
  title={Diversity and Generalization in Neural Network Ensembles},
  author={Ortega, Luis A and Caba{\~n}as, Rafael and Masegosa, Andr{\'e}s},
  booktitle={Proceedings of the 25th International Conference on Artificial Intelligence and Statistics},
  pages={11720--11743},
  year={2022},
  organization={PMLR}
}

@inproceedings{tekin2024llmtopla,
  title={LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity},
  author={Tekin, Selim Furkan and Ilhan, Fatih and Karakose, Ege and Kobas, Mert},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={5627--5642},
  year={2024}
}

@article{liu1999ensemble,
  title={Ensemble Learning via Negative Correlation},
  author={Liu, Yong and Yao, Xin},
  journal={Neural Networks},
  volume={12},
  number={10},
  pages={1399--1404},
  year={1999},
  publisher={Elsevier}
}

% Redundancy Reduction and Self-Supervised Learning
@inproceedings{zbontar2021barlow,
  title={Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  booktitle={Proceedings of the 38th International Conference on Machine Learning},
  pages={12310--12320},
  year={2021},
  organization={PMLR}
}

@inproceedings{bardes2022vicreg,
  title={VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
  author={Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{anonymous2024preventing,
  title={Preventing Dimensional Collapse via Orthogonality Regularization},
  author={Anonymous},
  journal={arXiv preprint arXiv:2411.00392},
  year={2024}
}

% Parameter-Efficient Fine-Tuning
@inproceedings{hu2022lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{wang2023loraensemble,
  title={LoRA Ensembles for Large Language Model Fine-Tuning},
  author={Wang, Xi Victoria and Ororbia, Alexander and Kini, Karthik and Lu, Yi},
  journal={arXiv preprint arXiv:2310.00035},
  year={2023}
}

@inproceedings{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages={4582--4597},
  year={2021}
}

@inproceedings{benzaken2022bitfit,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Ben Zaken, Elad and Goldberg, Yoav and Ravfogel, Shauli},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={1--9},
  year={2022}
}

@inproceedings{wen2020batchensemble,
  title={BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning},
  author={Wen, Yeming and Tran, Dustin and Ba, Jimmy},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{muhlematter2024loraensemble,
  title={LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention Networks},
  author={M{\"u}hlematter, Michelle and Sch{\"o}nenberger, Nils and He, Xingchen and Dubatovka, Alina and K{\"a}stner, Tilman},
  journal={arXiv preprint arXiv:2405.14438},
  year={2024}
}

% Edge Deployment and Small Language Models
@article{chen2024edge,
  title={A Review on Edge Large Language Models: Design, Execution, and Applications},
  author={Chen, Yu and Wang, Yufan and Zhang, Shengming and others},
  journal={arXiv preprint arXiv:2410.11845},
  year={2024}
}

% Inference-Time Scaling and Aggregation
@inproceedings{wang2022selfconsistency,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3714--3727},
  year={2022}
}

@article{stroebl2024inference,
  title={Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers},
  author={Stroebl, Justin and Antoniak, Manya and Choshen, Leshem},
  journal={arXiv preprint arXiv:2411.17501},
  year={2024}
}

@inproceedings{taubenfeld2025confidence,
  title={Confidence Improves Self-Consistency in Language Models},
  author={Taubenfeld, Yotam and Kotek, Hadas and Dagan, Ido},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2025},
  pages={1030--1045},
  year={2025}
}

@article{sanchez2023cfg,
  title={Stay on Topic with Classifier-Free Guidance},
  author={Sanchez, Guillaume and Fan, Honglu and Spangher, Alexander and Levi, Elad and Ammanamanchi, Pawan Sasanka and Biderman, Stella},
  journal={arXiv preprint arXiv:2306.17806},
  year={2023}
}

% Theoretical Foundations
@article{steffen2024misclassification,
  title={Misclassification Bounds for PAC-Bayesian Sparse Deep Learning},
  author={Steffen, Sonja and Scherrer, Benjamin and Hammer, Barbara},
  journal={Machine Learning},
  volume={113},
  pages={4679--4727},
  year={2024},
  publisher={Springer}
}

@inproceedings{biggs2022margins,
  title={On Margins and Derandomisation in PAC-Bayes},
  author={Biggs, Felix and Guedj, Benjamin},
  booktitle={Proceedings of the 25th International Conference on Artificial Intelligence and Statistics},
  pages={3709--3731},
  year={2022},
  organization={PMLR}
}

@article{alquier2024userfriendly,
  title={User-Friendly Introduction to PAC-Bayes Bounds},
  author={Alquier, Pierre},
  journal={Foundations and Trends in Machine Learning},
  volume={17},
  number={2},
  pages={174--303},
  year={2024},
  publisher={Now Publishers}
}

% Additional Hallucination References
@article{kalai2024calibrated,
  title={Calibrated Language Models Must Hallucinate},
  author={Kalai, Adam Tauman and Vempala, Santosh S.},
  journal={arXiv preprint arXiv:2311.14648},
  year={2024}
}

@article{yu2024mechanistic,
  title={Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations},
  author={Yu, Lei and Cao, Meng and Cheung, Jackie Chi Kit and Dong, Yue},
  journal={arXiv preprint arXiv:2403.18167},
  year={2024}
}

@article{manakul2023selfcheck,
  title={SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
  journal={arXiv preprint arXiv:2303.08896},
  year={2023}
}

@article{wei2024longform,
  title={Long-form Factuality in Large Language Models},
  author={Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Huang, Jie and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and Le, Quoc V.},
  journal={arXiv preprint arXiv:2403.18802},
  year={2024}
}

@article{ferrando2025knowledge,
  title={Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models},
  author={Ferrando, Javier and Obeso, Oscar and Rajamanoharan, Senthooran and Nanda, Neel},
  journal={arXiv preprint arXiv:2411.14257},
  year={2025}
}

% Enhanced Ensemble and Diversity References
@article{lakshminarayanan2017deep,
  title={Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{jing2022dimensional,
  title={Understanding Dimensional Collapse in Contrastive Self-supervised Learning},
  author={Jing, Li and Vincent, Pascal and LeCun, Yann and Tian, Yuandong},
  journal={arXiv preprint arXiv:2110.09348},
  year={2022}
}

% Evaluation Benchmarks
@article{kwiatkowski2019natural,
  title={Natural Questions: A Benchmark for Question Answering Research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={452--466},
  year={2019}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics},
  pages={1601--1611},
  year={2017}
}

@inproceedings{mallen2023trust,
  title={When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
  author={Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  year={2023}
}

@inproceedings{sakaguchi2020winogrande,
  title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={8732--8740},
  year={2020}
}

@article{merity2017pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2017}
}

@article{mckenzie2023inverse,
  title={Inverse Scaling: When Bigger Isn't Better},
  author={McKenzie, Ian R. and Lyzhov, Alexander and Pieler, Michael Martin and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Shen, Xudong and Cavanagh, Joe and Gritsevskiy, Andrew George and Kauffman, Derik and Kirtland, Aaron T. and Zhou, Zhengping and Zhang, Yuhui and Huang, Sicong and Wurgaft, Daniel and Weiss, Max and Ross, Alexis and Recchia, Gabriel and Liu, Alisa and Liu, Jiacheng and Tseng, Tom and Korbak, Tomasz and Kim, Najoung and Bowman, Samuel R. and Perez, Ethan},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
